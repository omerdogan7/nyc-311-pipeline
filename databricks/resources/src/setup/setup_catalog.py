# Databricks notebook source
# COMMAND ----------
"""
NYC 311 Pipeline Setup - Unity Catalog with External Volume
"""

from pyspark.sql import SparkSession

# COMMAND ----------

catalog = "nyc_311_dev"
raw_files_bucket = "s3://nyc-311-raw"     # â† Existing raw parquet files (read-only)
data_bucket = "s3://nyc-311-data-dev"     # â† DLT managed tables (write)

print(f"ğŸ“‹ Setup Configuration:")
print(f"  Catalog: {catalog}")
print(f"  Raw Files Bucket: {raw_files_bucket}")
print(f"  Data Tables Bucket: {data_bucket}")

# COMMAND ----------

# 1. Create Catalog with MANAGED LOCATION
print(f"\n1ï¸âƒ£ Creating catalog: {catalog}")

# First drop if exists to recreate with correct location
spark.sql(f"DROP CATALOG IF EXISTS {catalog} CASCADE")

spark.sql(f"""
    CREATE CATALOG {catalog}
    MANAGED LOCATION '{data_bucket}/'
    COMMENT 'NYC 311 Data Lakehouse - Managed by Unity Catalog'
""")

spark.sql(f"USE CATALOG {catalog}")
print("   âœ… Done")

# COMMAND ----------

# 2. Create Schemas
print(f"\n2ï¸âƒ£ Creating schemas...")

spark.sql(f"""
    CREATE SCHEMA IF NOT EXISTS bronze
    COMMENT 'Bronze layer - raw data tables'
""")

spark.sql(f"""
    CREATE SCHEMA IF NOT EXISTS silver
    COMMENT 'Silver layer - cleaned data'
""")

spark.sql(f"""
    CREATE SCHEMA IF NOT EXISTS gold
    COMMENT 'Gold layer - aggregated data'
""")
print("   âœ… Done")

# COMMAND ----------

# 3. Create External Volume
print(f"\n3ï¸âƒ£ Creating external volume...")

spark.sql("""
    CREATE EXTERNAL VOLUME IF NOT EXISTS bronze.raw_files
    LOCATION 's3://nyc-311-raw/'
    COMMENT 'External volume for NYC 311 raw parquet files'
""")

print("   âœ… Done")

# COMMAND ----------

# 4. Create Checkpoint Volumes
print(f"\n4ï¸âƒ£ Creating checkpoint volumes...")

spark.sql(f"CREATE VOLUME IF NOT EXISTS bronze.checkpoints")
spark.sql(f"CREATE VOLUME IF NOT EXISTS silver.checkpoints")
spark.sql(f"CREATE VOLUME IF NOT EXISTS gold.checkpoints")
print("   âœ… Done")

# COMMAND ----------

# 5. Verify Setup
print(f"\n5ï¸âƒ£ Verifying setup...")

print("\nğŸ“‹ Catalog Details:")
spark.sql(f"DESCRIBE CATALOG EXTENDED {catalog}").show(truncate=False)

print("\nğŸ“‹ Schemas:")
spark.sql(f"SHOW SCHEMAS IN {catalog}").show()

print("\nğŸ“‹ Volumes in Bronze:")
spark.sql("SHOW VOLUMES IN bronze").show()

print("\nğŸ“‹ External Volume Details:")
spark.sql("DESCRIBE VOLUME bronze.raw_files").show(truncate=False)

print(f"""
{'='*60}
âœ… SETUP COMPLETE - DLT READY!
{'='*60}

Catalog: {catalog}
Catalog Managed Location: {data_bucket}/

ğŸ“¦ EXTERNAL VOLUME (Read-Only):
/Volumes/{catalog}/bronze/raw_files/
  â†’ {raw_files_bucket}/
     â”œâ”€â”€ year=2010/           â† Existing parquet files
     â”‚   â”œâ”€â”€ file1.parquet
     â”‚   â””â”€â”€ ...
     â”œâ”€â”€ year=2011/
     â””â”€â”€ year=2025/

ğŸ“Š UNITY CATALOG MANAGED STORAGE (DLT Writes):
{data_bucket}/
â”œâ”€â”€ {catalog}.db/
â”‚   â”œâ”€â”€ bronze.db/       â† Bronze tables
â”‚   â”‚   â””â”€â”€ nyc_311_raw/
â”‚   â”œâ”€â”€ silver.db/       â† Silver tables
â”‚   â”‚   â””â”€â”€ nyc_311_cleaned/
â”‚   â””â”€â”€ gold.db/         â† Gold tables
â”‚       â””â”€â”€ complaint_summary/

ğŸ“ Checkpoint Volumes (Managed):
  â€¢ bronze.checkpoints
  â€¢ silver.checkpoints
  â€¢ gold.checkpoints

ğŸ”„ DLT Data Flow:
  READ  â†’ /Volumes/{catalog}/bronze/raw_files/year=*/
  WRITE â†’ {catalog}.bronze.nyc_311_raw (Delta table)
  WRITE â†’ {catalog}.silver.nyc_311_cleaned (Delta table)
  WRITE â†’ {catalog}.gold.complaint_summary (Delta table)

ğŸ“ Next Step:
   Use this path in DLT pipeline:
   source_path = "/Volumes/{catalog}/bronze/raw_files/"
{'='*60}
""")

# COMMAND ----------